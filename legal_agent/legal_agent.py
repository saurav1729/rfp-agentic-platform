# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vAqWoyOMXZzehRHqBzWFZLpcYmZ3mq4V
"""

# Cell 1 - Install required packages (run once)
!pip install -q google-generativeai PyPDF2 pdf2image pytesseract pillow numpy tqdm
# If you want a nicer progress bar for loops, tqdm is already included above.
print("Install step complete.")

# Cell 2 - Configure Gemini from Colab Secrets
from google.colab import userdata
import google.generativeai as genai

API_KEY = userdata.get("gemini_key")
if not API_KEY:
    raise RuntimeError("Gemini API key not found. Save it as a Colab Secret named 'gemini_key'")

genai.configure(api_key=API_KEY)
print("Gemini configured.")

# Cell 3 - Imports and utility functions
import os, json, re, uuid
import numpy as np
from tqdm import tqdm
from time import sleep
import PyPDF2
from pdf2image import convert_from_path
import pytesseract

def safe_truncate(s, n=800):
    return s if len(s) <= n else s[:n] + "..."

print("Utilities loaded.")

# Cell 4 - PDF -> text (PyPDF2 + OCR fallback)
def extract_text_from_pdf(path):
    text = ""
    # Try text extraction
    try:
        reader = PyPDF2.PdfReader(path)
        for p in reader.pages:
            t = p.extract_text()
            if t:
                text += t + "\n"
    except Exception as e:
        print("PyPDF2 read error (will try OCR):", e)

    # If extracted text is too short, fallback to OCR
    if len(text.strip()) < 50:
        try:
            imgs = convert_from_path(path, dpi=200)
            for img in imgs:
                text += pytesseract.image_to_string(img) + "\n"
        except Exception as e:
            print("OCR fallback failed:", e)

    return text

# Cell 5 - Chunking + cosine sim
def create_chunks(text, chunk_words=400, overlap_words=50):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = words[i:i+chunk_words]
        chunks.append(" ".join(chunk))
        i += (chunk_words - overlap_words)
    return chunks

def cosine_sim(a, b):
    a = np.array(a, dtype=np.float32)
    b = np.array(b, dtype=np.float32)
    if np.linalg.norm(a)==0 or np.linalg.norm(b)==0:
        return 0.0
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

# Cell 6 - Embedding wrapper using Gemini embeddings model
def get_embedding(text, model_name="models/text-embedding-004", retries=3):
    for attempt in range(retries):
        try:
            # Use genai.embed_content directly for embedding models
            emb_response = genai.embed_content(model=model_name, content=text)
            # genai.embed_content returns a dictionary with 'embedding' key
            return emb_response['embedding']
        except Exception as e:
            print(f"Embedding attempt {attempt+1} error: {e}")
            sleep(1 + attempt*2)
    raise RuntimeError("Failed to get embedding after retries")

# Cell 7 - Summarize and evaluate helper functions (uses Gemini text gen)
def genai_summarize(text, instruction="Summarize the following text in a concise, structured bullet list. Include main obligations, deadlines, acceptance criteria, and risks."):
    prompt = f"{instruction}\n\nText:\n{text}\n\nProvide a concise structured summary."
    model = genai.GenerativeModel("models/gemini-2.5-flash")
    resp = model.generate_content(prompt)
    return resp.text.strip()

def genai_evaluate_compliance(rfp_snippets, proposal_snippet, question_context=None):
    if question_context is None:
        question_context = ("Is the proposal excerpt compliant with the RFP requirements? "
                            "Answer with: Verdict (Compliant / Non-compliant). Then a 2-3 sentence justification. "
                            "If Non-compliant, bullet list missing / conflicting items from the RFP.")
    rfp_text = "\n\n---\n\n".join(rfp_snippets)
    prompt = f"""
You are a contract-review assistant.

RFP relevant excerpts:
{rfp_text}

Proposal excerpt:
{proposal_snippet}

Task:
{question_context}

Answer format (strict):
1) Verdict: Compliant / Non-compliant
2) Short justification (2-3 sentences)
3) If Non-compliant: bullet list of missing/contradicting RFP items
"""
    model = genai.GenerativeModel("models/gemini-2.5-flash")
    resp = model.generate_content(prompt)
    return resp.text.strip()

# Cell 8 - Semantic search (top-k by cosine similarity)
def semantic_search(query_emb, corpus_embeddings, top_k=5):
    scores = [(i, cosine_sim(query_emb, emb)) for i, emb in enumerate(corpus_embeddings)]
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[:top_k]

# Cell 9 - Main pipeline: process two PDFs and produce report
def run_legal_agent(rfp_pdf_path, proposal_pdf_path, chunk_words=400, top_k=5, similarity_threshold=0.55):
    print("Extracting RFP text...")
    rfp_text = extract_text_from_pdf(rfp_pdf_path)
    print("Extracting Proposal text...")
    proposal_text = extract_text_from_pdf(proposal_pdf_path)

    if len(rfp_text.strip()) < 20:
        raise RuntimeError("RFP extraction seems empty.")
    if len(proposal_text.strip()) < 20:
        raise RuntimeError("Proposal extraction seems empty.")

    print("Creating chunks...")
    rfp_chunks = create_chunks(rfp_text, chunk_words=chunk_words)
    proposal_chunks = create_chunks(proposal_text, chunk_words=chunk_words)

    print(f"RFP -> {len(rfp_chunks)} chunks, Proposal -> {len(proposal_chunks)} chunks")

    print("Embedding RFP chunks...")
    rfp_embeddings = [get_embedding(c) for c in tqdm(rfp_chunks)]

    print("Embedding Proposal chunks...")
    proposal_embeddings = [get_embedding(c) for c in tqdm(proposal_chunks)]

    # Summarize RFP (use a sample / top chunks to stay within prompt limits)
    print("Summarizing RFP (high-level)...")
    sample_for_summary = " ".join(rfp_chunks[:6]) if len(rfp_chunks) > 0 else rfp_text
    rfp_summary = genai_summarize(sample_for_summary,
                                  instruction="You are a contract analyst. Produce a clear summary of key requirements, deliverables, deadlines, acceptance criteria, and penalties.")

    print("\nRFP summary generated.")

    # Evaluate proposal chunks
    print("Evaluating proposal for compliance...")
    findings = []
    for idx, (p_chunk, p_emb) in enumerate(tqdm(list(zip(proposal_chunks, proposal_embeddings)), total=len(proposal_chunks))):
        top = semantic_search(p_emb, rfp_embeddings, top_k=top_k)
        top_indices = [i for i, s in top]
        top_scores = [s for i, s in top]
        if len(top_scores) == 0 or top_scores[0] < similarity_threshold:
            continue  # skip weak matches
        top_snippets = [rfp_chunks[i] for i in top_indices]
        evaluation = genai_evaluate_compliance(top_snippets, p_chunk)
        findings.append({
            "proposal_chunk_index": idx,
            "proposal_excerpt": safe_truncate(p_chunk, 800),
            "top_rfp_matches_indices": top_indices,
            "top_scores": top_scores,
            "evaluation": evaluation
        })

    # Aggregate simple verdict
    compliant_count = sum(1 for f in findings if "non-compliant" not in f["evaluation"].lower() and "compliant" in f["evaluation"].lower())
    non_compliant_count = sum(1 for f in findings if "non-compliant" in f["evaluation"].lower())
    total_checks = compliant_count + non_compliant_count
    if total_checks == 0:
        overall = "No strong matches found â€” cannot determine compliance automatically."
    else:
        pct_compliant = 100 * compliant_count / total_checks
        overall = f"{pct_compliant:.1f}% of matched proposal segments appear compliant ({compliant_count}/{total_checks}). {non_compliant_count} matched segments flagged non-compliant."

    report = {
        "rfp_summary": rfp_summary,
        "overall_verdict": overall,
        "detailed_findings": findings,
        "metadata": {
            "rfp_chunks": len(rfp_chunks),
            "proposal_chunks": len(proposal_chunks),
            "checks_run": total_checks
        }
    }
    return report

# Cell 10 (UPDATED) - Upload RFP and Proposal PDFs (use dialog), run the agent,
# and keep the report as an in-memory JSON-like Python object (do NOT write a file).
from google.colab import files
import json
import uuid

print("Upload the RFP PDF file (choose file dialog)...")
uploaded = files.upload()
if len(uploaded) == 0:
    raise RuntimeError("No file uploaded for RFP.")
rfp_file = list(uploaded.keys())[0]

print("Now upload the Proposal PDF file...")
uploaded2 = files.upload()
if len(uploaded2) == 0:
    raise RuntimeError("No file uploaded for Proposal.")
proposal_file = list(uploaded2.keys())[0]

print("Starting processing (this can take a while depending on doc size and API speed)...")
# run_legal_agent should be defined in earlier cells
report = run_legal_agent(rfp_file, proposal_file, chunk_words=400, top_k=5, similarity_threshold=0.55)

# --- Keep report as an in-memory Python dict (JSON object)
report_obj = report  # this is already a Python dict; you can use it directly

# Print concise outputs (same as before)
print("\n=== RFP Summary ===\n")
print(report_obj.get("rfp_summary", "No summary produced."), "\n")
print("=== Overall Verdict ===\n")
print(report_obj.get("overall_verdict", "No verdict produced."), "\n")

# Optionally show metadata and number of detailed findings
meta = report_obj.get("metadata", {})
print("Metadata:", meta)
print("Number of detailed findings:", len(report_obj.get("detailed_findings", [])))

# The report_obj variable now holds the full JSON-like object in memory.
# You can inspect it, pass it to other functions, convert to JSON string, or save later if needed.
# Example: convert to JSON string (in-memory)
report_json_string = json.dumps(report_obj, indent=2, ensure_ascii=False)

# If you want to preview the first 1000 characters of the JSON string:
print("\nPreview of JSON object (first 1000 chars):\n")
print(report_json_string[:1000])

# Cell 11 - Print detailed findings (first N)
N = 8
for i, f in enumerate(report["detailed_findings"][:N]):
    print(f"\n--- Finding {i+1} ---")
    print("Proposal excerpt (truncated):", f["proposal_excerpt"])
    print("Top RFP match indices:", f["top_rfp_matches_indices"])
    print("Top similarity scores:", [round(s,3) for s in f["top_scores"]])
    print("Evaluation by Gemini:\n", f["evaluation"])
    print("-"*60)